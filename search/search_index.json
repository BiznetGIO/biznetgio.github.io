{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"BiznetGio Docs BiznetGio Engineering Guide. BiznetGio Docs is built to serve as the developer guide to work with BiznetGio internal project or any FOSS external project that used by Biznetgio.","title":"Home"},{"location":"#biznetgio-docs","text":"BiznetGio Engineering Guide. BiznetGio Docs is built to serve as the developer guide to work with BiznetGio internal project or any FOSS external project that used by Biznetgio.","title":"BiznetGio Docs"},{"location":"about/contributing/","text":"Contributing to BiznetGio Docs Quickstart Install dependencies pip install - r requirements . txt Make your desired changes Review your changes with mkdocs serve Make pull request Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. Deploy documentation Info We use two branch. source to hold all the source contents, and master for compiled contents. mkdocs gh - deploy -- config - file ./ mkdocs . yml -- remote - branch master Or you can use CI automation: deploy : provider : pages skip_cleanup : true github_token : $github_token local_dir : site target_branch : master on : branch : source $github_token is the token than you generate from Github with the scope of public_repo and repo_deployment . Then put that token into your CI platform (such as Travis).","title":"Contributing"},{"location":"about/contributing/#contributing-to-biznetgio-docs","text":"","title":"Contributing to BiznetGio Docs"},{"location":"about/contributing/#quickstart","text":"Install dependencies pip install - r requirements . txt Make your desired changes Review your changes with mkdocs serve Make pull request","title":"Quickstart"},{"location":"about/contributing/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"about/contributing/#deploy-documentation","text":"Info We use two branch. source to hold all the source contents, and master for compiled contents. mkdocs gh - deploy -- config - file ./ mkdocs . yml -- remote - branch master Or you can use CI automation: deploy : provider : pages skip_cleanup : true github_token : $github_token local_dir : site target_branch : master on : branch : source $github_token is the token than you generate from Github with the scope of public_repo and repo_deployment . Then put that token into your CI platform (such as Travis).","title":"Deploy documentation"},{"location":"guide/bgn-dev-initiliaze/","text":"Intialize Project with BGN Developer Tools Tools support on Unix/Linux system or Debian and Redhat with all derivatives. Just going simple, after clone monorepo project from biznet gitlab type : ./bgn-dev init Give an access as root permissions when command prompt sudo appear. When initialize already done, you dont need to initialize again. Note If you use Windows 10 as your operating system, you can use WSL/WSL2 to run it. But some feature may not working perfectly(WSL under development). Run Sample Project Try run some existing project to ensure tools already installed and running. Example : Deploy rdap project on local development with command ./bgn-dev project deploy rdap Check running process ./bgn-dev project ps rdap Play to Next Step Tools packed with a bunch usefull command instructions , type help to see this. ./bgn-dev help or ./bgn-dev","title":"First Setup"},{"location":"guide/bgn-dev-initiliaze/#intialize-project-with-bgn-developer-tools","text":"Tools support on Unix/Linux system or Debian and Redhat with all derivatives. Just going simple, after clone monorepo project from biznet gitlab type : ./bgn-dev init Give an access as root permissions when command prompt sudo appear. When initialize already done, you dont need to initialize again. Note If you use Windows 10 as your operating system, you can use WSL/WSL2 to run it. But some feature may not working perfectly(WSL under development).","title":"Intialize Project with BGN Developer Tools"},{"location":"guide/bgn-dev-initiliaze/#run-sample-project","text":"Try run some existing project to ensure tools already installed and running. Example : Deploy rdap project on local development with command ./bgn-dev project deploy rdap Check running process ./bgn-dev project ps rdap","title":"Run Sample Project"},{"location":"guide/bgn-dev-initiliaze/#play-to-next-step","text":"Tools packed with a bunch usefull command instructions , type help to see this. ./bgn-dev help or ./bgn-dev","title":"Play to Next Step"},{"location":"guide/bgn-dev-manage-project/","text":"Show all project feature by typing command ./bgn-dev project Create New Project Create a project is very easy, use command ./bgn-dev project new awesome . You will guided with command prompt boilerplate option. Example from above, we do create a project as python boilerplate(option number 2). Project will automatically get prefix name with svc or as backend project. Info We define project with prefix svc as backend project and fe as frontend. All project created with bgn tools base from boilerplate that you choose. Running and Test On Local Development Run your project with ./bgn-dev project deploy svc-awesome . This step will proceed your container up on local with orcinus.","title":"Managing Project"},{"location":"guide/bgn-dev-manage-project/#create-new-project","text":"Create a project is very easy, use command ./bgn-dev project new awesome . You will guided with command prompt boilerplate option. Example from above, we do create a project as python boilerplate(option number 2). Project will automatically get prefix name with svc or as backend project. Info We define project with prefix svc as backend project and fe as frontend. All project created with bgn tools base from boilerplate that you choose.","title":"Create New Project"},{"location":"guide/bgn-dev-manage-project/#running-and-test-on-local-development","text":"Run your project with ./bgn-dev project deploy svc-awesome . This step will proceed your container up on local with orcinus.","title":"Running and Test On Local Development"},{"location":"guide/bgn-dev-remote-proxy/","text":"","title":"Bgn dev remote proxy"},{"location":"guide/bgn-dev-remote-tunnel/","text":"When service already deploy and run on staging, we can bind service on your local computers with tunneling. This method give many advantages: show all running up service and some informations on staging simulate our ready up staging service to local development make use of some staging service can access from local dev without installing service that nedeed access shared data or variables on staging store like redis, elastic, etc... Tunneling Concepts local dev <--> allowed docker container host <--> staging server host local dev is our computer for real, requesting a staging service through from allowed container allowed container host will forward to staging host, some permissions may disabled due for limitations access staging server accept request from allowed container host, back then forward the service from allowed container requested Command ./bgn-dev remote Show all running service on staging server First, show service that exist on staging to ensure list service ready up. bgn-dev remote list or bgn-dev remote ls This will return some informations like service name, port, container id and source addess of container. Tunnel Service Connect a staging project to local development(1 service) bgn-dev project remote <local port:staging_service_name:staging_port> local port is our local port computers, bind any port depending on your local available port staging_service_name can define from lookup NAME from remote list staging_port can define from lookup PORT from remote list, this port is from container service. Info if there are no port on informations list, you can find hardcoded on orcinus.yml or expose port on Dockerfile from base service name project. Example 1. Tunnel a service to local development Remote a service name giov2-api to local development. bgn-dev project remote 5858:giov2-api:5000 After success create tunneling, by curl or even open the browser and entry address localhost:port_tunnel 2. Tunnel more than one service to local development Some case you need more than one service can be used on local development. You can tunneling with sparated commas by commands: bgn-dev project remote <local port:staging_service_name:staging_port>,<local port:staging_service_name:staging_port>,etc...","title":"Tunneling Staging Apps"},{"location":"guide/bgn-dev-remote-tunnel/#tunneling-concepts","text":"local dev <--> allowed docker container host <--> staging server host local dev is our computer for real, requesting a staging service through from allowed container allowed container host will forward to staging host, some permissions may disabled due for limitations access staging server accept request from allowed container host, back then forward the service from allowed container requested","title":"Tunneling Concepts"},{"location":"guide/bgn-dev-remote-tunnel/#command","text":"./bgn-dev remote","title":"Command"},{"location":"guide/bgn-dev-remote-tunnel/#show-all-running-service-on-staging-server","text":"First, show service that exist on staging to ensure list service ready up. bgn-dev remote list or bgn-dev remote ls This will return some informations like service name, port, container id and source addess of container.","title":"Show all running service on staging server"},{"location":"guide/bgn-dev-remote-tunnel/#tunnel-service","text":"Connect a staging project to local development(1 service) bgn-dev project remote <local port:staging_service_name:staging_port> local port is our local port computers, bind any port depending on your local available port staging_service_name can define from lookup NAME from remote list staging_port can define from lookup PORT from remote list, this port is from container service. Info if there are no port on informations list, you can find hardcoded on orcinus.yml or expose port on Dockerfile from base service name project.","title":"Tunnel Service"},{"location":"guide/bgn-dev-remote-tunnel/#example","text":"","title":"Example"},{"location":"guide/bgn-dev-remote-tunnel/#1-tunnel-a-service-to-local-development","text":"Remote a service name giov2-api to local development. bgn-dev project remote 5858:giov2-api:5000 After success create tunneling, by curl or even open the browser and entry address localhost:port_tunnel","title":"1. Tunnel a service to local development"},{"location":"guide/bgn-dev-remote-tunnel/#2-tunnel-more-than-one-service-to-local-development","text":"Some case you need more than one service can be used on local development. You can tunneling with sparated commas by commands: bgn-dev project remote <local port:staging_service_name:staging_port>,<local port:staging_service_name:staging_port>,etc...","title":"2. Tunnel more than one service to local development"},{"location":"guide/bgn-dev-welcome/","text":"BGN Developer Tools Introduction We provide packed tools to manage your monorepo project on Gitlab with BGN Developer Tools . This tools help you create project from boilerplate, modifying and runing local development. Why we do use BGN Developer Tools? We recommended you to use BGN Developer Tools with some reason. Simple and Easy BGN Developer Tools run on bash shell script. Unix/Linux and its derivatives will run no issue. Boilerplate Project Create project from boilerplate, you can define whatever project that you want. Boilerplate also include many libraries writen many language(e.g. python, javascript, shell) Deploy and Run with Orcinus Orcinus is container orchestration management tools. All Around Code We keep all code in single repositories as monorepo. You can compare and learn code from another example project on folder. Integrated with Gitlab CI/CD Tools already integrated with autodevops, both staging and productions. When your code ready to publish, deliver your code with confidence. Code Analyst Automation Don't loose develop any project, keep always confidence to code. We already integrated with SonarQube , help you check how reability, secure and maintability your code.","title":"Prefix"},{"location":"guide/bgn-dev-welcome/#bgn-developer-tools","text":"","title":"BGN Developer Tools"},{"location":"guide/bgn-dev-welcome/#introduction","text":"We provide packed tools to manage your monorepo project on Gitlab with BGN Developer Tools . This tools help you create project from boilerplate, modifying and runing local development.","title":"Introduction"},{"location":"guide/bgn-dev-welcome/#why-we-do-use-bgn-developer-tools","text":"We recommended you to use BGN Developer Tools with some reason.","title":"Why we do use BGN Developer Tools?"},{"location":"guide/bgn-dev-welcome/#simple-and-easy","text":"BGN Developer Tools run on bash shell script. Unix/Linux and its derivatives will run no issue.","title":"Simple and Easy"},{"location":"guide/bgn-dev-welcome/#boilerplate-project","text":"Create project from boilerplate, you can define whatever project that you want. Boilerplate also include many libraries writen many language(e.g. python, javascript, shell)","title":"Boilerplate Project"},{"location":"guide/bgn-dev-welcome/#deploy-and-run-with-orcinus","text":"Orcinus is container orchestration management tools.","title":"Deploy and Run with Orcinus"},{"location":"guide/bgn-dev-welcome/#all-around-code","text":"We keep all code in single repositories as monorepo. You can compare and learn code from another example project on folder.","title":"All Around Code"},{"location":"guide/bgn-dev-welcome/#integrated-with-gitlab-cicd","text":"Tools already integrated with autodevops, both staging and productions. When your code ready to publish, deliver your code with confidence.","title":"Integrated with Gitlab CI/CD"},{"location":"guide/bgn-dev-welcome/#code-analyst-automation","text":"Don't loose develop any project, keep always confidence to code. We already integrated with SonarQube , help you check how reability, secure and maintability your code.","title":"Code Analyst Automation"},{"location":"guide/boilerplates/","text":"Project Boilerplates python-boilerplate https://github.com/BiznetGIO/python-boilerplate Project Structure This the the flask boilerplate that you can use to to kickstart your next REST API project. The app structure of this boilerplate is: . \u251c\u2500\u2500 app # application module \u2502 \u251c\u2500\u2500 configs \u2502 \u251c\u2500\u2500 controllers \u2502 \u2502 \u2514\u2500\u2500 api \u2502 \u251c\u2500\u2500 helpers \u2502 \u251c\u2500\u2500 libs \u2502 \u251c\u2500\u2500 middlewares \u2502 \u251c\u2500\u2500 models \u2502 \u2514\u2500\u2500 static \u251c\u2500\u2500 docker-compose.yml # docker compose config \u251c\u2500\u2500 Dockerfile # docker config \u251c\u2500\u2500 .dockerignore \u251c\u2500\u2500 .env.example # environment variable examples \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 README.md \u251c\u2500\u2500 requirements.txt # depedencies \u251c\u2500\u2500 run.sh \u2514\u2500\u2500 test configs : contains configuration variables. e.g API keys, database URIs, or variable of the application instance (things like DEBUG=True). You can ignore this if you are using docker environments. controllers/api : contains \"routes\" of the application. helpers : this directory act like \"helpers\" or \"utilitiles\" functions the applications libs : this directory act like \"vendor\", or any third-party module that you want to deliver within the application. middlewares : contains files that related to http request. e.g auth files. (you can ignore it at the beginning) models : contains models of the application. static : contains your static files .e.g images, javascript files. test : contains test suites against the application logic run.sh : contains files that will import the app and start the server. Quickstart To start using this boilerplate: # install required dependencies $ pip install -r requirements.txt # run the app python manage.py server","title":"Project Boilerplates"},{"location":"guide/boilerplates/#project-boilerplates","text":"","title":"Project Boilerplates"},{"location":"guide/boilerplates/#python-boilerplate","text":"https://github.com/BiznetGIO/python-boilerplate","title":"python-boilerplate"},{"location":"guide/boilerplates/#project-structure","text":"This the the flask boilerplate that you can use to to kickstart your next REST API project. The app structure of this boilerplate is: . \u251c\u2500\u2500 app # application module \u2502 \u251c\u2500\u2500 configs \u2502 \u251c\u2500\u2500 controllers \u2502 \u2502 \u2514\u2500\u2500 api \u2502 \u251c\u2500\u2500 helpers \u2502 \u251c\u2500\u2500 libs \u2502 \u251c\u2500\u2500 middlewares \u2502 \u251c\u2500\u2500 models \u2502 \u2514\u2500\u2500 static \u251c\u2500\u2500 docker-compose.yml # docker compose config \u251c\u2500\u2500 Dockerfile # docker config \u251c\u2500\u2500 .dockerignore \u251c\u2500\u2500 .env.example # environment variable examples \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 README.md \u251c\u2500\u2500 requirements.txt # depedencies \u251c\u2500\u2500 run.sh \u2514\u2500\u2500 test configs : contains configuration variables. e.g API keys, database URIs, or variable of the application instance (things like DEBUG=True). You can ignore this if you are using docker environments. controllers/api : contains \"routes\" of the application. helpers : this directory act like \"helpers\" or \"utilitiles\" functions the applications libs : this directory act like \"vendor\", or any third-party module that you want to deliver within the application. middlewares : contains files that related to http request. e.g auth files. (you can ignore it at the beginning) models : contains models of the application. static : contains your static files .e.g images, javascript files. test : contains test suites against the application logic run.sh : contains files that will import the app and start the server.","title":"Project Structure"},{"location":"guide/boilerplates/#quickstart","text":"To start using this boilerplate: # install required dependencies $ pip install -r requirements.txt # run the app python manage.py server","title":"Quickstart"},{"location":"guide/common-errors/","text":"Common Errors Docker ERROR: Couldn't connect to Docker daemon at http+docker://localunixsocket - is it running? Add your current user to docker group then activates new permissions for docker group sudo usermod -a -G docker $USER newgrp docker Virtual machine can't do ssh or scp to CentOS remote vm Change PasswordAuthentication in /etc/ssh/sshd_config 'PasswordAuthentication no' # to 'PasswordAuthentication yes' then restart the service sudo systemctl restart sshd","title":"Common Errors"},{"location":"guide/common-errors/#common-errors","text":"","title":"Common Errors"},{"location":"guide/common-errors/#docker","text":"ERROR: Couldn't connect to Docker daemon at http+docker://localunixsocket - is it running? Add your current user to docker group then activates new permissions for docker group sudo usermod -a -G docker $USER newgrp docker","title":"Docker"},{"location":"guide/common-errors/#virtual-machine","text":"can't do ssh or scp to CentOS remote vm Change PasswordAuthentication in /etc/ssh/sshd_config 'PasswordAuthentication no' # to 'PasswordAuthentication yes' then restart the service sudo systemctl restart sshd","title":"Virtual machine"},{"location":"guide/container-production-enviroment/","text":"Modern deployment need take fast and efficient manageable process workflow. Meanwhile, containerize apps one of solutions to driven fast software delivery. Making as containerize make legacy apps running without limitations from enviroment hosts, reliable delivery and scalable runtime. Containerize apps not enough when apps ready to deliver. Common issued appear when developer spend many time to setup their applications, it will getting worst if company has contain many project. We introduce Container Production Environment(CPE) to manage our deployment. This CPE stage adopt from modern software development lifecycle, aim for simplify entire our project run and deploy. CPE enviroment scope build and running of the application using an existing orcinus file. Means, it ready support with AutoDevops concept, CPE will run automatically detect, build, test, deploy and monitor base on where stage you deploy, staging or production. Orcinus Orcinus is opensource simple orchestration management tools already support k8s (kubernetes). With orcinus file present configurations, container image, service name, enviroment and some configurations can handled infrastructure within code/config. Infrastructure as Code (IaC) Orcinus already support to describe your infrastructure within orcinus yaml file. It means, you allowed to managing your infrastructure operations environment by adjusting whatever you applied. Look some sample figure here. Figure 1 Node 1, Node 2, Node 3 and Node-n describe a physical server configurations connected with a single network manager. We define stack enviroment named Orcinus Stack . When you on same stack, you allowed access other service name just by call their service name. In other words, same stack is same as in network manager. Figure 2 In other case, we need to allowed your service connect and access in public internet. Set traefik port same with enviroment variables. And setup your enviroment domain same with traefik.frontend.rule. Feel interest about orcinus tools? Check our wiki here Orcinus file on monorepo project Orcinus file with extension yaml format (.yml) required to run with orcinus tools. Container Production Enviroment as AutoDevops will automatically detect to deploy and running a container service. Generated orcinus file Then inside your project, we already generated 3 orcinus file. orcinus-dev.yml : orcinus file to run on your local development orcinus.yml : orcinus file to run on staging orcinus-production.yml : orcinus file to run on productions Structure monorepo project . your-awesomeproject # monorepo project \u251c\u2500\u2500 <generated from boilerplate code> \u251c\u2500\u2500 orcinus-dev.yml \u251c\u2500\u2500 orcinus.yml \u251c\u2500\u2500 orcinus-production.yml \u2514\u2500\u2500 version Note File version is not auto generated, you need to add this when ready to production. Change this file with version base on your history release production. Orcinus yml file example This file sample from generated orcinus-dev.yml stack: \"orcinus\" services: my-awesome-project: image: my-awesome-project:dev ports: - \"5000:5000\" environment: - API_URL=\"http://foo/api/v1\" - API_KEY=\"This secret key\" - API_PORT=5000 commands: [\"sh\",\"run.sh\", \"server\", \"development\"] stack : name of stack, if a service up in a same stack you can access by call the service name. all configurations on services define their service itself include variables, enviroment, script and so on. image: my-awesome-project:dev : name of container image that will be running on deployment ports : expose virtual port to host port environment : some var related to setup or service configurations commands : run service, this step should do when you running the service By default, a service run with orcinus will act like auto-healing on kubernetes. You can prevent this with add restart_policy: condition: \"none\" Credits https://github.com/orcinustools","title":"Container Production Enviroment"},{"location":"guide/container-production-enviroment/#orcinus","text":"Orcinus is opensource simple orchestration management tools already support k8s (kubernetes). With orcinus file present configurations, container image, service name, enviroment and some configurations can handled infrastructure within code/config.","title":"Orcinus"},{"location":"guide/container-production-enviroment/#infrastructure-as-code-iac","text":"Orcinus already support to describe your infrastructure within orcinus yaml file. It means, you allowed to managing your infrastructure operations environment by adjusting whatever you applied. Look some sample figure here.","title":"Infrastructure as Code (IaC)"},{"location":"guide/container-production-enviroment/#figure-1","text":"Node 1, Node 2, Node 3 and Node-n describe a physical server configurations connected with a single network manager. We define stack enviroment named Orcinus Stack . When you on same stack, you allowed access other service name just by call their service name. In other words, same stack is same as in network manager.","title":"Figure 1"},{"location":"guide/container-production-enviroment/#figure-2","text":"In other case, we need to allowed your service connect and access in public internet. Set traefik port same with enviroment variables. And setup your enviroment domain same with traefik.frontend.rule. Feel interest about orcinus tools? Check our wiki here","title":"Figure 2"},{"location":"guide/container-production-enviroment/#orcinus-file-on-monorepo-project","text":"Orcinus file with extension yaml format (.yml) required to run with orcinus tools. Container Production Enviroment as AutoDevops will automatically detect to deploy and running a container service.","title":"Orcinus file on monorepo project"},{"location":"guide/container-production-enviroment/#generated-orcinus-file","text":"Then inside your project, we already generated 3 orcinus file. orcinus-dev.yml : orcinus file to run on your local development orcinus.yml : orcinus file to run on staging orcinus-production.yml : orcinus file to run on productions","title":"Generated orcinus file"},{"location":"guide/container-production-enviroment/#structure-monorepo-project","text":". your-awesomeproject # monorepo project \u251c\u2500\u2500 <generated from boilerplate code> \u251c\u2500\u2500 orcinus-dev.yml \u251c\u2500\u2500 orcinus.yml \u251c\u2500\u2500 orcinus-production.yml \u2514\u2500\u2500 version Note File version is not auto generated, you need to add this when ready to production. Change this file with version base on your history release production.","title":"Structure monorepo project"},{"location":"guide/container-production-enviroment/#orcinus-yml-file-example","text":"This file sample from generated orcinus-dev.yml stack: \"orcinus\" services: my-awesome-project: image: my-awesome-project:dev ports: - \"5000:5000\" environment: - API_URL=\"http://foo/api/v1\" - API_KEY=\"This secret key\" - API_PORT=5000 commands: [\"sh\",\"run.sh\", \"server\", \"development\"] stack : name of stack, if a service up in a same stack you can access by call the service name. all configurations on services define their service itself include variables, enviroment, script and so on. image: my-awesome-project:dev : name of container image that will be running on deployment ports : expose virtual port to host port environment : some var related to setup or service configurations commands : run service, this step should do when you running the service By default, a service run with orcinus will act like auto-healing on kubernetes. You can prevent this with add restart_policy: condition: \"none\"","title":"Orcinus yml file example"},{"location":"guide/container-production-enviroment/#credits","text":"https://github.com/orcinustools","title":"Credits"},{"location":"guide/contrib-guide/","text":"Guidelines Project organization We only use one branch for the project: master branch. Tag will be created to mark stable release. Bug fix or Hot fix branches should be created for fixing bugs and merged into master when ready. Info we don't use master - development branch for monorepo. Instead we use <servicename>/<workitem> such as api/add-health-endpoint , and those branches will be merged into master eventually. Project name in repositories We define project name with prefix svc/fe in repositories. svc- <yourawesomeproject-api> If your project contain backend APi such a microservice. fe- <yourawesomeproject-frontend> If a frontend project. <yourawesomeproject> Left blank if doesn't contain frontend or backend. Opening a new issue Look through existing issues to see if your issue already exists. So we don't have duplicate issue . If your issue already exists, comment on its thread with any information you have. Even if this is simply to note that you are having the same problem, it is still helpful! Always be as descriptive as you can . What is the expected behavior? What is the actual behavior? What are the steps to reproduce? Attach screenshots, videos, GIFs if possible. Include <project> version or branch experiencing the issue. Include OS version experiencing the issue. Submitting a pull request Find an issue to work on, or create a new one. Avoid duplicates, please check existing issues! Fork the repo, or make sure you are synced with the latest changes on master . Create a new branch with a sweet name: git checkout -b project-name/[issue/work items] . Do code. Plese follow PEP8 Please watch your line length . It's advised to limit under 80 char. Write unit tests when applicable. Don't break unit tests or functionality. Update the documentation header comments if needed. Rebase on master branch and resolve any conflicts before submitting a pull request! Submit a pull request to the master branch. Make sure to add yourself to AUTHORS file. Info Start the title with WIP: to prevent a Work In Progress merge request from being merged before it's ready. Close issue Tag an issue start with close #<issue_number> on your pull request description. Issue will close automatically when pull request merged succesfully. Restricted Rule Dont create new branch except from master. Dont create pull request except to master. Define prefix WIP: to avoid project owner merge your code if you not done yet. Perhaps if you still change your code and unmerge pull request, edit your pull request title with prefix WIP: You should not commit and push your code to others existing pull request. Do it for your pull request only! Important Creating new branch without rebasing from master will miss some latest code. Before continue on your working branch, always fetch first and rebase with master. Still confuse with this? Check git tips and ticks here . First time setup Please refer to instalation guide and running test suitein each project.","title":"Guidelines"},{"location":"guide/contrib-guide/#guidelines","text":"","title":"Guidelines"},{"location":"guide/contrib-guide/#project-organization","text":"We only use one branch for the project: master branch. Tag will be created to mark stable release. Bug fix or Hot fix branches should be created for fixing bugs and merged into master when ready. Info we don't use master - development branch for monorepo. Instead we use <servicename>/<workitem> such as api/add-health-endpoint , and those branches will be merged into master eventually.","title":"Project organization"},{"location":"guide/contrib-guide/#project-name-in-repositories","text":"We define project name with prefix svc/fe in repositories. svc- <yourawesomeproject-api> If your project contain backend APi such a microservice. fe- <yourawesomeproject-frontend> If a frontend project. <yourawesomeproject> Left blank if doesn't contain frontend or backend.","title":"Project name in repositories"},{"location":"guide/contrib-guide/#opening-a-new-issue","text":"Look through existing issues to see if your issue already exists. So we don't have duplicate issue . If your issue already exists, comment on its thread with any information you have. Even if this is simply to note that you are having the same problem, it is still helpful! Always be as descriptive as you can . What is the expected behavior? What is the actual behavior? What are the steps to reproduce? Attach screenshots, videos, GIFs if possible. Include <project> version or branch experiencing the issue. Include OS version experiencing the issue.","title":"Opening a new issue"},{"location":"guide/contrib-guide/#submitting-a-pull-request","text":"Find an issue to work on, or create a new one. Avoid duplicates, please check existing issues! Fork the repo, or make sure you are synced with the latest changes on master . Create a new branch with a sweet name: git checkout -b project-name/[issue/work items] . Do code. Plese follow PEP8 Please watch your line length . It's advised to limit under 80 char. Write unit tests when applicable. Don't break unit tests or functionality. Update the documentation header comments if needed. Rebase on master branch and resolve any conflicts before submitting a pull request! Submit a pull request to the master branch. Make sure to add yourself to AUTHORS file. Info Start the title with WIP: to prevent a Work In Progress merge request from being merged before it's ready.","title":"Submitting a pull request"},{"location":"guide/contrib-guide/#close-issue","text":"Tag an issue start with close #<issue_number> on your pull request description. Issue will close automatically when pull request merged succesfully.","title":"Close issue"},{"location":"guide/contrib-guide/#restricted-rule","text":"Dont create new branch except from master. Dont create pull request except to master. Define prefix WIP: to avoid project owner merge your code if you not done yet. Perhaps if you still change your code and unmerge pull request, edit your pull request title with prefix WIP: You should not commit and push your code to others existing pull request. Do it for your pull request only! Important Creating new branch without rebasing from master will miss some latest code. Before continue on your working branch, always fetch first and rebase with master. Still confuse with this? Check git tips and ticks here .","title":"Restricted Rule"},{"location":"guide/contrib-guide/#first-time-setup","text":"Please refer to instalation guide and running test suitein each project.","title":"First time setup"},{"location":"guide/contrib-intro/","text":"Contributing to open source BiznetGio Project We'd be happy for you to contribute! Collaborate with us We welcome you become collaborator on our project. watch our repositories on github to keep watching any update. Give star rate if feel interest to find out related topic later. Well, very nice if you fork and contribute your code as collaborator. Support questions Please, don't use the issue tracker for this. Use the preserved gitter channel in each project.","title":"Intro"},{"location":"guide/contrib-intro/#contributing-to-open-source-biznetgio-project","text":"We'd be happy for you to contribute!","title":"Contributing to open source BiznetGio Project"},{"location":"guide/contrib-intro/#collaborate-with-us","text":"We welcome you become collaborator on our project. watch our repositories on github to keep watching any update. Give star rate if feel interest to find out related topic later. Well, very nice if you fork and contribute your code as collaborator.","title":"Collaborate with us"},{"location":"guide/contrib-intro/#support-questions","text":"Please, don't use the issue tracker for this. Use the preserved gitter channel in each project.","title":"Support questions"},{"location":"guide/deploy-app/","text":"Deploying Application Create your Virtual Machine login into https://horizon.neo.id/ . Ask the credential to your team. create new vm: go to \"compute \u2192 instances\", select \"launch instance\" fill appropriate \"instance name\" and other fields. choose CentOS in \"source\" for the sake of similarity. choose your flavour size ask your team what \"network\" to use. ask your team what \"security group\" to use. ask your team what \"keypair\" to use. Ignore \"network prort\", \"configuration\", \"server group\", \"scheduler\", and \"metadata\" for now. select \"launch instance\" Get your private key get the \"private-key\" by visiting \"orchestration \u2192 stack \u2192 your-vm-key \u2192 overview\" copy that \"private-key\" to write that to a file with appropriate name in your local machine. e.g key.pem run chmod 600 key.pem Accessing your VM Do it with ssh: ssh -i /path/to/your/vm-key.pem <your-vm-username>@<public-ip> then do your business there. Commonly installing docker and docker-compose sudo yum install docker # installing docker # installing docker-compose (yes, it's not in centos repo) sudo yum install epel-release sudo yum install -y python-pip sudo pip install docker-compose Taking your image to your vm Note It's advicable to avoid using Docker Hub for our development image. It's also not suitable way of doing managing internal projects. Use scp for the win! Sadly, scp will not work seamlessly in centos . You need to change: 'PasswordAuthentication no' # to 'PasswordAuthentication yes' in \"/etc/ssh/sshd_config\", then re-started the service sudo systemctl restart sshd Prepare your image to be transferred using SCP save docker as tar docker save -o <path for generated tar file> <image name> move from your local machine to remote vm scp -i /path/to/key.pem whois-api.tar <username>@<public-ip>:~/your/destination/dir/ cd to your \"destionation/dir\" then load the image docker load -i <path to image tar file> Run the app Run the app as you did in local machine $ docker-compose up","title":"Deploy App on Devbox"},{"location":"guide/deploy-app/#deploying-application","text":"","title":"Deploying Application"},{"location":"guide/deploy-app/#create-your-virtual-machine","text":"login into https://horizon.neo.id/ . Ask the credential to your team. create new vm: go to \"compute \u2192 instances\", select \"launch instance\" fill appropriate \"instance name\" and other fields. choose CentOS in \"source\" for the sake of similarity. choose your flavour size ask your team what \"network\" to use. ask your team what \"security group\" to use. ask your team what \"keypair\" to use. Ignore \"network prort\", \"configuration\", \"server group\", \"scheduler\", and \"metadata\" for now. select \"launch instance\"","title":"Create your Virtual Machine"},{"location":"guide/deploy-app/#get-your-private-key","text":"get the \"private-key\" by visiting \"orchestration \u2192 stack \u2192 your-vm-key \u2192 overview\" copy that \"private-key\" to write that to a file with appropriate name in your local machine. e.g key.pem run chmod 600 key.pem","title":"Get your private key"},{"location":"guide/deploy-app/#accessing-your-vm","text":"Do it with ssh: ssh -i /path/to/your/vm-key.pem <your-vm-username>@<public-ip> then do your business there. Commonly installing docker and docker-compose sudo yum install docker # installing docker # installing docker-compose (yes, it's not in centos repo) sudo yum install epel-release sudo yum install -y python-pip sudo pip install docker-compose","title":"Accessing your VM"},{"location":"guide/deploy-app/#taking-your-image-to-your-vm","text":"Note It's advicable to avoid using Docker Hub for our development image. It's also not suitable way of doing managing internal projects. Use scp for the win! Sadly, scp will not work seamlessly in centos . You need to change: 'PasswordAuthentication no' # to 'PasswordAuthentication yes' in \"/etc/ssh/sshd_config\", then re-started the service sudo systemctl restart sshd","title":"Taking your image to your vm"},{"location":"guide/deploy-app/#prepare-your-image-to-be-transferred-using-scp","text":"save docker as tar docker save -o <path for generated tar file> <image name> move from your local machine to remote vm scp -i /path/to/key.pem whois-api.tar <username>@<public-ip>:~/your/destination/dir/ cd to your \"destionation/dir\" then load the image docker load -i <path to image tar file>","title":"Prepare your image to be transferred using SCP"},{"location":"guide/deploy-app/#run-the-app","text":"Run the app as you did in local machine $ docker-compose up","title":"Run the app"},{"location":"guide/deployment-production/","text":"Production is final stage when distributed and deliver built applications going public. Surely, all functional feature already checked and tested by product owner. Production Phase Same with staging phase deployment, on production we do with autodevops on gitlab ci. Production pipeline will triggered when project owner or deployment team accept your pull request merge to master branch. We determine with 3 stage process on production pipeline : build release deploy 1. build Jobs will build final contarize image from Dockerfile and tagged same with file version on your project folder. File version convention with semantic versioning 2.0.0, concern this note when you change it. example : 2 = major 1 = minor 2 = patch Given a version number MAJOR.MINOR.PATCH, increment the: 1. MAJOR version when you make incompatible API changes, 2. MINOR version when you add functionality in a backwards compatible manner, and 3. PATCH version when you make backwards compatible bug fixes. Additional labels for pre-release and build metadata are available as extensions to the MAJOR.MINOR.PATCH format. reference: semantic version v2.0.0 Atention Please keep mind, file version needed to change if you ready to production, dont change if not ready deploy to production. 2. release Release stage is push/upload successfull built container image to our registry image. 3. deploy Deploying will automatically detect orcinus-production.yml , this will run on container production enviroment. Production Deployment Note On monorepo project, change your version on file version On legacy repositories, tag your version and commit push to new version Given tag <release_date>-<increment_number> example : 19092020-01 19092020 = release date 19 September 2020 01 = release number 1 base on date Assignee your project owner on your pull request to review it","title":"Deployment Production"},{"location":"guide/deployment-production/#production-phase","text":"Same with staging phase deployment, on production we do with autodevops on gitlab ci. Production pipeline will triggered when project owner or deployment team accept your pull request merge to master branch. We determine with 3 stage process on production pipeline : build release deploy","title":"Production Phase"},{"location":"guide/deployment-production/#1-build","text":"Jobs will build final contarize image from Dockerfile and tagged same with file version on your project folder. File version convention with semantic versioning 2.0.0, concern this note when you change it. example : 2 = major 1 = minor 2 = patch Given a version number MAJOR.MINOR.PATCH, increment the: 1. MAJOR version when you make incompatible API changes, 2. MINOR version when you add functionality in a backwards compatible manner, and 3. PATCH version when you make backwards compatible bug fixes. Additional labels for pre-release and build metadata are available as extensions to the MAJOR.MINOR.PATCH format. reference: semantic version v2.0.0 Atention Please keep mind, file version needed to change if you ready to production, dont change if not ready deploy to production.","title":"1. build"},{"location":"guide/deployment-production/#2-release","text":"Release stage is push/upload successfull built container image to our registry image.","title":"2. release"},{"location":"guide/deployment-production/#3-deploy","text":"Deploying will automatically detect orcinus-production.yml , this will run on container production enviroment.","title":"3. deploy"},{"location":"guide/deployment-production/#production-deployment-note","text":"On monorepo project, change your version on file version On legacy repositories, tag your version and commit push to new version Given tag <release_date>-<increment_number> example : 19092020-01 19092020 = release date 19 September 2020 01 = release number 1 base on date Assignee your project owner on your pull request to review it","title":"Production Deployment Note"},{"location":"guide/deployment-staging/","text":"We define 2 stage deploy project to public, staging and production. Staging is internal release, this phase process to test and review by product owner. After apps ready successfully runnning on staging, all feature will be check according with product documentations. This step make sure of all functional feature works normally before move to productions. Staging Phase Autodevops deployment staging process by pipeline gitlab ci/cd . Pipeline are the top-level component of continuous integration, delivery, and deployment. Pipeline comprise with jobs and stage. Job define what process todo, and stage define when process to do. In our Staging, we determine 4 step stage. Test Build Release Deploy If either one or some stage fail, another next stage can not be proceed and the pipeline ends early. Some conditions on fail, you can retry a stage to rerun jobs execute again. To monitoring jobs execute, detailed can be view as linux terminal(tty) on browser. 1. test Test stage is doing jobs verify with internal test package and sonarqube. Common project microservice/backend python include with testing code, pytest or unittest to testing functional and libraries. By default test stage will send project to sonarqube, output can be viewed on our sonarqube site . 2. build Build stage is step when containerize the applications. Jobs build container image base on Dockerfile that include on your project folder. 3. release Release stage is step push/upload successfull container image to our registry image. This step register an image container with tag latest and ready to deploy on staging server. 4. deploy This final step to running your containerize applications on staging server. It will automatically detect orcinus.yml on container production enviroment and up onto staging server. Staging Deployment Note Already publish code to staging please do this : Create pull request without prefix WIP: List your workitems on descriptions. Give tag close #issue_number to close automatically after pull request accepted. Assignee your team to review your code Meanwhile if you dont want deploy on staging : Only push your code to your active branch Dont create pull request to master If you already create pull request on git, edit your pull request with title WIP:","title":"Deployment Staging"},{"location":"guide/deployment-staging/#staging-phase","text":"Autodevops deployment staging process by pipeline gitlab ci/cd . Pipeline are the top-level component of continuous integration, delivery, and deployment. Pipeline comprise with jobs and stage. Job define what process todo, and stage define when process to do. In our Staging, we determine 4 step stage. Test Build Release Deploy If either one or some stage fail, another next stage can not be proceed and the pipeline ends early. Some conditions on fail, you can retry a stage to rerun jobs execute again. To monitoring jobs execute, detailed can be view as linux terminal(tty) on browser.","title":"Staging Phase"},{"location":"guide/deployment-staging/#1-test","text":"Test stage is doing jobs verify with internal test package and sonarqube. Common project microservice/backend python include with testing code, pytest or unittest to testing functional and libraries. By default test stage will send project to sonarqube, output can be viewed on our sonarqube site .","title":"1. test"},{"location":"guide/deployment-staging/#2-build","text":"Build stage is step when containerize the applications. Jobs build container image base on Dockerfile that include on your project folder.","title":"2. build"},{"location":"guide/deployment-staging/#3-release","text":"Release stage is step push/upload successfull container image to our registry image. This step register an image container with tag latest and ready to deploy on staging server.","title":"3. release"},{"location":"guide/deployment-staging/#4-deploy","text":"This final step to running your containerize applications on staging server. It will automatically detect orcinus.yml on container production enviroment and up onto staging server.","title":"4. deploy"},{"location":"guide/deployment-staging/#staging-deployment-note","text":"Already publish code to staging please do this : Create pull request without prefix WIP: List your workitems on descriptions. Give tag close #issue_number to close automatically after pull request accepted. Assignee your team to review your code Meanwhile if you dont want deploy on staging : Only push your code to your active branch Dont create pull request to master If you already create pull request on git, edit your pull request with title WIP:","title":"Staging Deployment Note"},{"location":"guide/dockerize-flask/","text":"Dockerzie Flask Application Docker Clash Course [ start ] sudo systemctl start docker ( debian based ) sudo service docker { stop | start | restart } ( centos ) [ build ] docker build -t whois-api:latest . [ List container ] docker ps -a ## list all docker ps -l ## list latest image [ List image ] docker image ls [ remove ] docker rmi -f $( docker images -f \"dangling=true\" -q ) ## daggling images Docker Intro Make sure you have installed docker and docker-compose. Verify your installation by checking their version Add the Dockerfile ## we choose alpine for the sake of tiny size FROM python:3.7-alpine RUN pip3 install gunicorn ## wsgi server in your container ## choose 'app' as workdir. Docker will create if it doesn't exists WORKDIR /app # copy just the requirements.txt first to leverage Docker cache COPY ./requirements.txt /app/requirements.txt RUN pip3 install -r /app/requirements.txt COPY . /app ## define the port you will be using EXPOSE 5000 Read Dockerfilereference documentation to learn more about these keywords. To learn more about Alpine OS apk command read Alpine Linux package management wiki . Leveraging Docker build cache documented in Dockerfile Bestpractice Build the image docker build -t flask-tutorial:0.0.1 . - -t : to tag your image name - . : current dir (bash syntax) Running the container For the sake of ease, we will use docker-compose to run the container. Note Please discuss with your team, what port you will be using and what OS variable that you need (if you use environment keyword) Then, add the appropriate docker-compose.yml file: ## remove anyline like this, I am just a description version : '3' services : whois : ## your container name image : flask-tutorial:0.0.1 ## your image name (that you built earlier) with the tag ports : ## the port on your `HOST:CONTAINER` - \"5000:5000\" environment : ## discuss with your team what to put here - APP_HOST=0.0.0.0 - APP_PORT=5000 - WHOIS_KEY=fakekey123 command : sh run.sh server production Read Docker Compose documentation to learn more about these keyword. Run the app when everything ok $ docker-compose up","title":"Dockerize Flask App"},{"location":"guide/dockerize-flask/#dockerzie-flask-application","text":"","title":"Dockerzie Flask Application"},{"location":"guide/dockerize-flask/#docker-clash-course","text":"[ start ] sudo systemctl start docker ( debian based ) sudo service docker { stop | start | restart } ( centos ) [ build ] docker build -t whois-api:latest . [ List container ] docker ps -a ## list all docker ps -l ## list latest image [ List image ] docker image ls [ remove ] docker rmi -f $( docker images -f \"dangling=true\" -q ) ## daggling images","title":"Docker Clash Course"},{"location":"guide/dockerize-flask/#docker-intro","text":"Make sure you have installed docker and docker-compose. Verify your installation by checking their version Add the Dockerfile ## we choose alpine for the sake of tiny size FROM python:3.7-alpine RUN pip3 install gunicorn ## wsgi server in your container ## choose 'app' as workdir. Docker will create if it doesn't exists WORKDIR /app # copy just the requirements.txt first to leverage Docker cache COPY ./requirements.txt /app/requirements.txt RUN pip3 install -r /app/requirements.txt COPY . /app ## define the port you will be using EXPOSE 5000 Read Dockerfilereference documentation to learn more about these keywords. To learn more about Alpine OS apk command read Alpine Linux package management wiki . Leveraging Docker build cache documented in Dockerfile Bestpractice","title":"Docker Intro"},{"location":"guide/dockerize-flask/#build-the-image","text":"docker build -t flask-tutorial:0.0.1 . - -t : to tag your image name - . : current dir (bash syntax)","title":"Build the image"},{"location":"guide/dockerize-flask/#running-the-container","text":"For the sake of ease, we will use docker-compose to run the container. Note Please discuss with your team, what port you will be using and what OS variable that you need (if you use environment keyword) Then, add the appropriate docker-compose.yml file: ## remove anyline like this, I am just a description version : '3' services : whois : ## your container name image : flask-tutorial:0.0.1 ## your image name (that you built earlier) with the tag ports : ## the port on your `HOST:CONTAINER` - \"5000:5000\" environment : ## discuss with your team what to put here - APP_HOST=0.0.0.0 - APP_PORT=5000 - WHOIS_KEY=fakekey123 command : sh run.sh server production Read Docker Compose documentation to learn more about these keyword. Run the app when everything ok $ docker-compose up","title":"Running the container"},{"location":"guide/git-tutorial/","text":"Here we give some tips and tricks when you work in a team to manage git project. 1. Always create branch from master. Tips: Do as always before you create new branch. Untracking file without sync from master probably raise conflicts. Tricks: checkout to master first git checkout master pull from upstream to update all content and informations into your local repo git pull you can create branch then checkout to new branch git checkout -b <your-new-feature-branch> 2. Keep your branch with latest code. Tips: please do always rebase your current active branch with master. Why you should do this? with rebase your branch will always sync with master if any changed or merged. Tricks: on your current working branch do this step git fetch then rebase from master git rebase origin/master do pull to ensure your code sync with master git pull","title":"Git Tips and Tricks"},{"location":"guide/git-tutorial/#1-always-create-branch-from-master","text":"Tips: Do as always before you create new branch. Untracking file without sync from master probably raise conflicts. Tricks: checkout to master first git checkout master pull from upstream to update all content and informations into your local repo git pull you can create branch then checkout to new branch git checkout -b <your-new-feature-branch>","title":"1. Always create branch from master."},{"location":"guide/git-tutorial/#2-keep-your-branch-with-latest-code","text":"Tips: please do always rebase your current active branch with master. Why you should do this? with rebase your branch will always sync with master if any changed or merged. Tricks: on your current working branch do this step git fetch then rebase from master git rebase origin/master do pull to ensure your code sync with master git pull","title":"2. Keep your branch with latest code."},{"location":"guide/gunicorn/","text":"Gunicorn Best Practice Prefix Gunicorn is a Python WSGI HTTP Server that usually lives between a reverse proxy (e.g., Nginx) or load balancer (e.g., AWS ELB) and a web application such as Django or Flask. Gunicorn Improve Performance To improve performance when using Gunicorn we have to keep in mind 3 means of concurrency. 1. workers. Basic fundamental for handling concurrency process using by workers. The suggested number of workers is (2*CPU)+1 . If we are using a dual-core(2 CPU) machine, 5 is the suggested workers value. gunicorn --workers = 5 main:app 2. threads on workers. Each workers allow have multiple threads when python application running. If we implement threads to workers, by default worker class set is to gthread . The suggested maximum concurrent requests when using workers and threads is still (2*CPU)+1 . If we are using a dual-core(2 CPU) machine and we want to use a mix of workers and threads, we could use 5 workers and 1 threads, to get 5 maximum concurrent requests. gunicorn --workers = 5 --threads = 1 main:app it same as : gunicorn --workers = 5 --threads = 1 --worker-class = gthread main:app 3. pseudo-threads. There are some Python libraries such as gevent and Asyncio that enable concurrency in Python by using pseudo-threads implemented with coroutines. Gunicorn allows for the usage of these asynchronous Python libraries by setting their corresponding worker class. Here the settings that would work for a single core machine that we want to run using gevent: gunicorn --worker-class = gevent --worker-connections = 1000 --workers = 3 main:app Info worker-connections is a specific setting for the gevent worker class. See also worker class on https://docs.gunicorn.org/en/stable/settings.html#worker-class (2*CPU)+1 is still the suggested workers since we only have 1 core, we\u2019ll be using 3 workers. In this case, the maximum number of concurrent requests is 3000 (3 workers * 1000 connections per worker) Credits https://medium.com/building-the-system/gunicorn-3-means-of-concurrency-efbb547674b7 https://docs.gunicorn.org/en/stable/settings.html#worker-processes","title":"Gunicorn Best Practice"},{"location":"guide/gunicorn/#gunicorn-best-practice","text":"","title":"Gunicorn Best Practice"},{"location":"guide/gunicorn/#prefix","text":"Gunicorn is a Python WSGI HTTP Server that usually lives between a reverse proxy (e.g., Nginx) or load balancer (e.g., AWS ELB) and a web application such as Django or Flask.","title":"Prefix"},{"location":"guide/gunicorn/#gunicorn-improve-performance","text":"To improve performance when using Gunicorn we have to keep in mind 3 means of concurrency.","title":"Gunicorn Improve Performance"},{"location":"guide/gunicorn/#1-workers","text":"Basic fundamental for handling concurrency process using by workers. The suggested number of workers is (2*CPU)+1 . If we are using a dual-core(2 CPU) machine, 5 is the suggested workers value. gunicorn --workers = 5 main:app","title":"1. workers."},{"location":"guide/gunicorn/#2-threads-on-workers","text":"Each workers allow have multiple threads when python application running. If we implement threads to workers, by default worker class set is to gthread . The suggested maximum concurrent requests when using workers and threads is still (2*CPU)+1 . If we are using a dual-core(2 CPU) machine and we want to use a mix of workers and threads, we could use 5 workers and 1 threads, to get 5 maximum concurrent requests. gunicorn --workers = 5 --threads = 1 main:app it same as : gunicorn --workers = 5 --threads = 1 --worker-class = gthread main:app","title":"2. threads on workers."},{"location":"guide/gunicorn/#3-pseudo-threads","text":"There are some Python libraries such as gevent and Asyncio that enable concurrency in Python by using pseudo-threads implemented with coroutines. Gunicorn allows for the usage of these asynchronous Python libraries by setting their corresponding worker class. Here the settings that would work for a single core machine that we want to run using gevent: gunicorn --worker-class = gevent --worker-connections = 1000 --workers = 3 main:app Info worker-connections is a specific setting for the gevent worker class. See also worker class on https://docs.gunicorn.org/en/stable/settings.html#worker-class (2*CPU)+1 is still the suggested workers since we only have 1 core, we\u2019ll be using 3 workers. In this case, the maximum number of concurrent requests is 3000 (3 workers * 1000 connections per worker)","title":"3. pseudo-threads."},{"location":"guide/gunicorn/#credits","text":"https://medium.com/building-the-system/gunicorn-3-means-of-concurrency-efbb547674b7 https://docs.gunicorn.org/en/stable/settings.html#worker-processes","title":"Credits"},{"location":"guide/integrations-test/","text":"Integrations test is internal procedural check component or functional based on an analysis of product. White box testing will do in this step. Whether your project include with unit testing, you can add to integrated test on gitlab ci. This will automate to test your functional code before release on staging. Some note, it still ready support for python project such as microservice project. Adding test for pre-served Makefile Add this line below to your Makefile. .PHONY: test test: sudo apk add libxml2-dev libxslt-dev sudo pip3 install -r requirements.txt sudo pip3 install -r requirements-dev.txt pytest -vv -s <project_test_folder> Change <project_test_folder> with your unit testing folder. Example Project Contain Unit Testing Here explain a python project ready support execute integrated tools on ci/cd. figure 1 : example a unit testing figure 2 : Makefile file Output can be viewed as terminal linux(tty) on browser when pipeline test started. figure 3 : automated test running on ci/cd","title":"Integrations Test"},{"location":"guide/integrations-test/#adding-test-for-pre-served-makefile","text":"Add this line below to your Makefile. .PHONY: test test: sudo apk add libxml2-dev libxslt-dev sudo pip3 install -r requirements.txt sudo pip3 install -r requirements-dev.txt pytest -vv -s <project_test_folder> Change <project_test_folder> with your unit testing folder.","title":"Adding test for pre-served Makefile"},{"location":"guide/integrations-test/#example-project-contain-unit-testing","text":"Here explain a python project ready support execute integrated tools on ci/cd. figure 1 : example a unit testing figure 2 : Makefile file Output can be viewed as terminal linux(tty) on browser when pipeline test started. figure 3 : automated test running on ci/cd","title":"Example Project Contain Unit Testing"},{"location":"guide/knot-clash-course/","text":"Knot Clash Course Quickstart installation: $ sudo apt-get install knot run knot: $ sudo systemctl start knot # to check if it's running $ sudo systemctl status knot Server login copy the key and put it into a key.pem file change it's mode: chmod 600 key.pem get your server ip (.e.g from your horizon portal) log to your machine using ssh $ ssh -i key.pem centos@<your ip master> Accessible machine (the one that had the public Ips) are only the slaves. To access the servers that had private IPs (including master). Use the key file inside the slave servers. to log to server that didn't have public IPs. [ centos@cmg01z00knsl001 ~ ] $ ls # this in in master server STAGING STAGING.tar.gz vm-key.pem [ centos@cmg01z00knsl001 ~ ] $ ssh -i vm-key.pem centos@<slave ip> [ centos@vultr-test-1 ~ ] $ # logged into slave server We use the following format, to name our servers: <serverlocation | servernumber | purpose | rule | number> e.g: cmg01z00knms001: for master cmg01z00knsl001: for slave Knot setups Ensure all old files removed Warning Do this only if you knot what you are doing, otherwise jump directly to \"start adding zone\" at the next step\" rm -rf * /var/lib/knot # knot database rm -rf * /etc/knot # knot config rm -rf * /run/knot # knot socket start the knot # systemctl start knot The initial file of knot (after removing all contents in /var/lib/knot/ and /etc/knot/ ) are only the timers directory in /var/lib/knot/ start adding zone (or other things) sudo knotc conf-init Start knotc conf-init . This step only requered if you have fresh knot installation. This will add confdb directory in /var/lib/knot/ . To check the content of knot database, you can do export it to the file: # knotc conf-export /etc/knot/knot.conf # to check it's content # cat /etc/knot/knot.conf Adding config (simple example) # knotc conf-begin ## start every command with `*-begin` OK # knotc conf-set server.version \"None of your bussiness\" OK # knotc conf-set server.listen \"0.0.0.0@53\" OK # knotc conf-commit ## end every command with `*-commit` OK # knotc conf-read # to see knot db content server.version = None of your bussiness server.listen = 0 .0.0.0@53 This will put the value to knot database. To export it to file (e.g knot.conf) you can do knotc conf-export /etc/knot/knot.conf . But entering value one by one using knotc is tedious. You can add them to a file in: /etc/knot.conf then import to the knot db: knotc conf-import knot.conf Adding zone (SOA example) # knotc zone-begin lapar.io # knotc zone-set lapar.io. @ 86400 SOA ns1.biz.net.id. hostmaster.biz.net.id. 2018070410 10800 3600 604800 38400 # knotc zone-commit lapar.io OK # check if it's okay # knotc zone-read lapar.io [ lapar.io. ] lapar.io. 86400 SOA ns1.biz.net.id. hostmaster.biz.net.id. 2018070410 10800 3600 604800 38400 Adding zone (NS example) # knotc zone-begin lapar.io OK # knotc zone-set lapar.io. @ 14000 NS ns1.biznet.id. OK # knotc zone-commit lapar.io # knotc zone-read lapar.io [ lapar.io. ] lapar.io. 14000 NS ns1.biznet.id. [ lapar.io. ] lapar.io. 86400 SOA ns1.biz.net.id. hostmaster.biz.net.id. 2018070411 10800 3600 604800 38400 Adding zone (A example) # knotc zone-begin lapar.io OK # knotc zone-set lapar.io. @ 3600 A 127.0.0.1 # knotc zone-commit lapar.io Testing the result kdig @localhost lapar.io SOA +tcp kdig @localhost lapar.io A +tcp Additional information To learn more consult knot documentation","title":"Knot Clash Course"},{"location":"guide/knot-clash-course/#knot-clash-course","text":"","title":"Knot Clash Course"},{"location":"guide/knot-clash-course/#quickstart","text":"installation: $ sudo apt-get install knot run knot: $ sudo systemctl start knot # to check if it's running $ sudo systemctl status knot","title":"Quickstart"},{"location":"guide/knot-clash-course/#server-login","text":"copy the key and put it into a key.pem file change it's mode: chmod 600 key.pem get your server ip (.e.g from your horizon portal) log to your machine using ssh $ ssh -i key.pem centos@<your ip master> Accessible machine (the one that had the public Ips) are only the slaves. To access the servers that had private IPs (including master). Use the key file inside the slave servers. to log to server that didn't have public IPs. [ centos@cmg01z00knsl001 ~ ] $ ls # this in in master server STAGING STAGING.tar.gz vm-key.pem [ centos@cmg01z00knsl001 ~ ] $ ssh -i vm-key.pem centos@<slave ip> [ centos@vultr-test-1 ~ ] $ # logged into slave server We use the following format, to name our servers: <serverlocation | servernumber | purpose | rule | number> e.g: cmg01z00knms001: for master cmg01z00knsl001: for slave","title":"Server login"},{"location":"guide/knot-clash-course/#knot-setups","text":"Ensure all old files removed Warning Do this only if you knot what you are doing, otherwise jump directly to \"start adding zone\" at the next step\" rm -rf * /var/lib/knot # knot database rm -rf * /etc/knot # knot config rm -rf * /run/knot # knot socket start the knot # systemctl start knot The initial file of knot (after removing all contents in /var/lib/knot/ and /etc/knot/ ) are only the timers directory in /var/lib/knot/ start adding zone (or other things) sudo knotc conf-init Start knotc conf-init . This step only requered if you have fresh knot installation. This will add confdb directory in /var/lib/knot/ . To check the content of knot database, you can do export it to the file: # knotc conf-export /etc/knot/knot.conf # to check it's content # cat /etc/knot/knot.conf","title":"Knot setups"},{"location":"guide/knot-clash-course/#adding-config-simple-example","text":"# knotc conf-begin ## start every command with `*-begin` OK # knotc conf-set server.version \"None of your bussiness\" OK # knotc conf-set server.listen \"0.0.0.0@53\" OK # knotc conf-commit ## end every command with `*-commit` OK # knotc conf-read # to see knot db content server.version = None of your bussiness server.listen = 0 .0.0.0@53 This will put the value to knot database. To export it to file (e.g knot.conf) you can do knotc conf-export /etc/knot/knot.conf . But entering value one by one using knotc is tedious. You can add them to a file in: /etc/knot.conf then import to the knot db: knotc conf-import knot.conf","title":"Adding config (simple example)"},{"location":"guide/knot-clash-course/#adding-zone-soa-example","text":"# knotc zone-begin lapar.io # knotc zone-set lapar.io. @ 86400 SOA ns1.biz.net.id. hostmaster.biz.net.id. 2018070410 10800 3600 604800 38400 # knotc zone-commit lapar.io OK # check if it's okay # knotc zone-read lapar.io [ lapar.io. ] lapar.io. 86400 SOA ns1.biz.net.id. hostmaster.biz.net.id. 2018070410 10800 3600 604800 38400","title":"Adding zone (SOA example)"},{"location":"guide/knot-clash-course/#adding-zone-ns-example","text":"# knotc zone-begin lapar.io OK # knotc zone-set lapar.io. @ 14000 NS ns1.biznet.id. OK # knotc zone-commit lapar.io # knotc zone-read lapar.io [ lapar.io. ] lapar.io. 14000 NS ns1.biznet.id. [ lapar.io. ] lapar.io. 86400 SOA ns1.biz.net.id. hostmaster.biz.net.id. 2018070411 10800 3600 604800 38400","title":"Adding zone (NS example)"},{"location":"guide/knot-clash-course/#adding-zone-a-example","text":"# knotc zone-begin lapar.io OK # knotc zone-set lapar.io. @ 3600 A 127.0.0.1 # knotc zone-commit lapar.io","title":"Adding zone (A example)"},{"location":"guide/knot-clash-course/#testing-the-result","text":"kdig @localhost lapar.io SOA +tcp kdig @localhost lapar.io A +tcp","title":"Testing the result"},{"location":"guide/knot-clash-course/#additional-information","text":"To learn more consult knot documentation","title":"Additional information"},{"location":"guide/knot-setups/","text":"RESTKnot Setup Installations Install etcd & knot apt install knot libknot8 etcd Start knot & etcd sudo systemctl start etcd sudo systemctl start knot # check if they are running sudo systemctl status etcd sudo systemctl status knot Run kafka & zookeeper (using docker container) create file docker-compose.yml which contains: version: '3' services: zookeeper: image: wurstmeister/zookeeper kafka: image: wurstmeister/kafka ports: - \"9092:9092\" environment: KAFKA_ADVERTISED_HOST_NAME: localhost KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 Then run it with: docker-compose up Setups Setup knot sudo knotc conf-init # trigger knot db to save configs Setup the RESTKnot agent # create venv mkvirtualenv rest-knot --python = /usr/bin/python3.7 # install agent dependencies cd RESTKnot/agent/ pip install -r requirements.txt pip install -e . Set broker and knot value sudo /home/azzamsya/.virtualenvs/rest-knot/bin/dnsagent envi broker sudo /home/azzamsya/.virtualenvs/rest-knot/bin/dnsagent envi knot Value example: Info Each operating system has different location of libknot.so and it's socket, so check it carefully. # knot env value example OS_KNOT_LIB = libknot.so OS_KNOT_SOCKS = /var/run/knot/knot.sock # broker env value example OS_BROKER = localhost OS_PORTS = 9092 OS_TOPIC = domaindata OS_FLAGS = master OS_GROUP = cmgz_master Check value sudo /home/azzamsya/.virtualenvs/rest-knot/bin/dnsagent envi show -e broker Start the agent sudo /home/azzamsya/.virtualenvs/rest-knot/bin/dnsagent start master Setup the RESTKnot API Install API dependencies cd RESTKnot/api/ pip install -r requirements.txt pip install -e . Populate data to etcd cd RESTKnot/api/ python migrate_db.py Run RESTKnot API # for development sh run.sh server development # for production sh run.sh server production Test the result $ curl 127 .0.0.1:5000/api/zone/list { \"code\" :200, \"count\" :1, \"data\" : [ { \"key\" : \"1\" , \"value\" : \"example1.com\" , \"created_at\" : \"2019-09-30 20:05:13.486640\" , \"user\" : { \"key\" : \"1\" , \"email\" : \"admin@foo.com\" , \"project_id\" : \"001\" , \"state\" : \"inserted\" , \"created_at\" : \"2019-07-20 23:04:22.420505\" } } ] , \"Status\" : \"success\" , \"message\" : \"Operation succeeded\" }","title":"RESTKnot Setup"},{"location":"guide/knot-setups/#restknot-setup","text":"","title":"RESTKnot Setup"},{"location":"guide/knot-setups/#installations","text":"Install etcd & knot apt install knot libknot8 etcd Start knot & etcd sudo systemctl start etcd sudo systemctl start knot # check if they are running sudo systemctl status etcd sudo systemctl status knot Run kafka & zookeeper (using docker container) create file docker-compose.yml which contains: version: '3' services: zookeeper: image: wurstmeister/zookeeper kafka: image: wurstmeister/kafka ports: - \"9092:9092\" environment: KAFKA_ADVERTISED_HOST_NAME: localhost KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 Then run it with: docker-compose up","title":"Installations"},{"location":"guide/knot-setups/#setups","text":"","title":"Setups"},{"location":"guide/knot-setups/#setup-knot","text":"sudo knotc conf-init # trigger knot db to save configs","title":"Setup knot"},{"location":"guide/knot-setups/#setup-the-restknot-agent","text":"# create venv mkvirtualenv rest-knot --python = /usr/bin/python3.7 # install agent dependencies cd RESTKnot/agent/ pip install -r requirements.txt pip install -e . Set broker and knot value sudo /home/azzamsya/.virtualenvs/rest-knot/bin/dnsagent envi broker sudo /home/azzamsya/.virtualenvs/rest-knot/bin/dnsagent envi knot Value example: Info Each operating system has different location of libknot.so and it's socket, so check it carefully. # knot env value example OS_KNOT_LIB = libknot.so OS_KNOT_SOCKS = /var/run/knot/knot.sock # broker env value example OS_BROKER = localhost OS_PORTS = 9092 OS_TOPIC = domaindata OS_FLAGS = master OS_GROUP = cmgz_master Check value sudo /home/azzamsya/.virtualenvs/rest-knot/bin/dnsagent envi show -e broker Start the agent sudo /home/azzamsya/.virtualenvs/rest-knot/bin/dnsagent start master","title":"Setup the RESTKnot agent"},{"location":"guide/knot-setups/#setup-the-restknot-api","text":"Install API dependencies cd RESTKnot/api/ pip install -r requirements.txt pip install -e . Populate data to etcd cd RESTKnot/api/ python migrate_db.py Run RESTKnot API # for development sh run.sh server development # for production sh run.sh server production","title":"Setup the RESTKnot API"},{"location":"guide/knot-setups/#test-the-result","text":"$ curl 127 .0.0.1:5000/api/zone/list { \"code\" :200, \"count\" :1, \"data\" : [ { \"key\" : \"1\" , \"value\" : \"example1.com\" , \"created_at\" : \"2019-09-30 20:05:13.486640\" , \"user\" : { \"key\" : \"1\" , \"email\" : \"admin@foo.com\" , \"project_id\" : \"001\" , \"state\" : \"inserted\" , \"created_at\" : \"2019-07-20 23:04:22.420505\" } } ] , \"Status\" : \"success\" , \"message\" : \"Operation succeeded\" }","title":"Test the result"},{"location":"guide/logger-dashboard/","text":"We provide logs to tracking your project running on server. Produce logs contain with standard out and error logs messages. So may this will help you debug code when getting some issues. Staging Logs Staging logs can access by remote logs with bgn tools. This will help you check some error logs as realtime on terminal. First find service name with remote list command. Already explain from this tutorial: bgn dev show remote service Then do with this command below: ./bgn-dev remote logs <service_name> Production Logs We don't produce stream production logs like on staging. You can request from gitlab as pipeline and logs will viewed as jobs tty. 1. Click button pipeline to create a new pipeline job 2. Set variabel with SERVICE_NAME with value service name. This service name can be found in orcinus-production.yml 3. Pipeline job will created and procced 4. Pipeline will show logs service name from production. Click button complete raw to show logs as text detail. Due limit size of jobs logs, you can only download 500kb file logs.","title":"Logs"},{"location":"guide/logger-dashboard/#staging-logs","text":"Staging logs can access by remote logs with bgn tools. This will help you check some error logs as realtime on terminal. First find service name with remote list command. Already explain from this tutorial: bgn dev show remote service Then do with this command below: ./bgn-dev remote logs <service_name>","title":"Staging Logs"},{"location":"guide/logger-dashboard/#production-logs","text":"We don't produce stream production logs like on staging. You can request from gitlab as pipeline and logs will viewed as jobs tty. 1. Click button pipeline to create a new pipeline job 2. Set variabel with SERVICE_NAME with value service name. This service name can be found in orcinus-production.yml 3. Pipeline job will created and procced 4. Pipeline will show logs service name from production. Click button complete raw to show logs as text detail. Due limit size of jobs logs, you can only download 500kb file logs.","title":"Production Logs"},{"location":"guide/managing-ci-cd/","text":"Deploy applications on development may need takes up time. Become unusefull when the jobs be like repetitive works.CI/CD concepts ideology solve this with execute some script and automate on running machine. CI short from Continuous Integration, this practice when developers finish their code push and merge on online repository. Developers can shared their changes code in several times for a sometime CD short from Continuous Delivery, this automate the entire applications release process. and other means CD is Continuous Deploy, on gitlab this automate when applications ready serve and running onto server (may staging or production) refence : gitlab ci/cd We do here works with gitlab, we provide integrate tools with gitlab ci/cd. This present short a simplicity to manage create gitlab-ci.yaml without limitations. bgn-dev ci Use command with bgn-dev tools ./bgn-dev ci listing project stage To make sure of your project ready on autodevops enviroment, check with bgn-dev tools ./bgn-dev ci list <your-awesome-project> example show listed project svc-ms-gio-v2 already registered on gitlab-ci: ./bgn-dev ci list svc-ms-gio-v2 This will print output : gitlab-ci for Test Generate a gitlab-ci file test with bgn-dev tools ./bgn-dev ci generate-test <your-awesome-project> gitlab-ci for Staging Generate a gitlab-ci file staging with bgn-dev tools ./bgn-dev ci generate-staging <your-awesome-project> gitlab-ci for Production Generate a gitlab-ci file production with bgn-dev tools ./bgn-dev ci generate-production <your-awesome-project> gitlab-ci rebuild some case conflicted gitlab-ci.yml you can rebuild with command: ./bgn-dev ci rebuild Important on monorepo project please contact deployment team to generate gitlab ci file to avoid conflict file","title":"Managing CI/CD"},{"location":"guide/managing-ci-cd/#bgn-dev-ci","text":"Use command with bgn-dev tools ./bgn-dev ci","title":"bgn-dev ci"},{"location":"guide/managing-ci-cd/#listing-project-stage","text":"To make sure of your project ready on autodevops enviroment, check with bgn-dev tools ./bgn-dev ci list <your-awesome-project> example show listed project svc-ms-gio-v2 already registered on gitlab-ci: ./bgn-dev ci list svc-ms-gio-v2 This will print output :","title":"listing project stage"},{"location":"guide/managing-ci-cd/#gitlab-ci-for-test","text":"Generate a gitlab-ci file test with bgn-dev tools ./bgn-dev ci generate-test <your-awesome-project>","title":"gitlab-ci for Test"},{"location":"guide/managing-ci-cd/#gitlab-ci-for-staging","text":"Generate a gitlab-ci file staging with bgn-dev tools ./bgn-dev ci generate-staging <your-awesome-project>","title":"gitlab-ci for Staging"},{"location":"guide/managing-ci-cd/#gitlab-ci-for-production","text":"Generate a gitlab-ci file production with bgn-dev tools ./bgn-dev ci generate-production <your-awesome-project>","title":"gitlab-ci for Production"},{"location":"guide/managing-ci-cd/#gitlab-ci-rebuild","text":"some case conflicted gitlab-ci.yml you can rebuild with command: ./bgn-dev ci rebuild Important on monorepo project please contact deployment team to generate gitlab ci file to avoid conflict file","title":"gitlab-ci rebuild"},{"location":"guide/reporting-workflow/","text":"On internal development we do syncup meeting on every wednesday. Happily all reporting in our internal development automated by telegram bot. Don't be afraid lost of tracking your issue, bot already record your activity from task board. Bot will send reporting file with extension csv and markdown for human readable. Don't left an issue for 5 days within any progress. Discuss with your team to solve the problem. Note Don't forget move your issue to next step board when progress already done Finnaly, just focus your task. Make your days nice and fun with coding. Let us do automate your progress activity !","title":"Report"},{"location":"guide/security-conventions/","text":"This section provide instructions to comply with the standard convention how to secure code in development. For Code: 1. Validate input. Validate input from all untrusted data sources. Proper input validation can eliminate the vast majority of software vulnerabilities. Be suspicious of most external data sources, including command line arguments, network interfaces, environmental variables, and user controlled files. 2. Architect and design for security policies. Create a software architecture and design your software to implement and enforce security policies. For example, if your system requires different privileges at different times, consider dividing the system into distinct intercommunicating subsystems, each with an appropriate privilege set. 3. Keep it simple. Keep the design as simple and small as possible. Complex designs increase the likelihood that errors will be made in their implementation, configuration, and use. Additionally, the effort required to achieve an appropriate level of assurance increases dramatically as security mechanisms become more complex. 4. Default deny. Base access decisions on permission rather than exclusion. This means that, by default, access is denied and the protection scheme identifies conditions under which access is permitted. Adhere to the principle of least privilege. Every process should execute with the the least set of privileges necessary to complete the job. Any elevated permission should only be accessed for the least amount of time required to complete the privileged task. This approach reduces the opportunities an attacker has to execute arbitrary code with elevated privileges. 5. Sanitize data sent to other systems. Sanitize all data passed to complex subsystems such as command shells, relational databases, and commercial off-the-shelf (COTS) components. Attackers may be able to invoke unused functionality in these components through the use of SQL, command, or other injection attacks. This is not necessarily an input validation problem because the complex subsystem being invoked does not understand the context in which the call is made. Because the calling process understands the context, it is responsible for sanitizing the data before invoking the subsystem. 6. Practice defense in depth. Manage risk with multiple defensive strategies, so that if one layer of defense turns out to be inadequate, another layer of defense can prevent a security flaw from becoming an exploitable vulnerability and/or limit the consequences of a successful exploit. For example, combining secure programming techniques with secure runtime environments should reduce the likelihood that vulnerabilities remaining in the code at deployment time can be exploited in the operational environment. 7. Use effective quality assurance techniques. Good quality assurance techniques can be effective in identifying and eliminating vulnerabilities. Fuzz testing, penetration testing, and source code audits should all be incorporated as part of an effective quality assurance program. Independent security reviews can lead to more secure systems. External reviewers bring an independent perspective; for example, in identifying and correcting invalid assumptions. For Databases: As for SQL, generate your SQL as close to the database - and as far from the user - as possible. UX-side apps should use data-requesting APIs to backend apps that generate SQL, and they should never generate SQL anywhere where a front-end user can see any part of it. If they even have direct connections to the database, you're probably exposed. Never generate SQL in front-end code running on a browser. You're providing a nice interface with already-authenticated connections to hackers that goes directly to your database. To avoid SQL injection attacks, it is best to use prepared statements versus direct execution at your SQL layer, even for queries that run once.","title":"Security Conventions"},{"location":"guide/security-conventions/#for-code","text":"","title":"For Code:"},{"location":"guide/security-conventions/#1-validate-input","text":"Validate input from all untrusted data sources. Proper input validation can eliminate the vast majority of software vulnerabilities. Be suspicious of most external data sources, including command line arguments, network interfaces, environmental variables, and user controlled files.","title":"1. Validate input."},{"location":"guide/security-conventions/#2-architect-and-design-for-security-policies","text":"Create a software architecture and design your software to implement and enforce security policies. For example, if your system requires different privileges at different times, consider dividing the system into distinct intercommunicating subsystems, each with an appropriate privilege set.","title":"2. Architect and design for security policies."},{"location":"guide/security-conventions/#3-keep-it-simple","text":"Keep the design as simple and small as possible. Complex designs increase the likelihood that errors will be made in their implementation, configuration, and use. Additionally, the effort required to achieve an appropriate level of assurance increases dramatically as security mechanisms become more complex.","title":"3. Keep it simple."},{"location":"guide/security-conventions/#4-default-deny","text":"Base access decisions on permission rather than exclusion. This means that, by default, access is denied and the protection scheme identifies conditions under which access is permitted. Adhere to the principle of least privilege. Every process should execute with the the least set of privileges necessary to complete the job. Any elevated permission should only be accessed for the least amount of time required to complete the privileged task. This approach reduces the opportunities an attacker has to execute arbitrary code with elevated privileges.","title":"4. Default deny."},{"location":"guide/security-conventions/#5-sanitize-data-sent-to-other-systems","text":"Sanitize all data passed to complex subsystems such as command shells, relational databases, and commercial off-the-shelf (COTS) components. Attackers may be able to invoke unused functionality in these components through the use of SQL, command, or other injection attacks. This is not necessarily an input validation problem because the complex subsystem being invoked does not understand the context in which the call is made. Because the calling process understands the context, it is responsible for sanitizing the data before invoking the subsystem.","title":"5. Sanitize data sent to other systems."},{"location":"guide/security-conventions/#6-practice-defense-in-depth","text":"Manage risk with multiple defensive strategies, so that if one layer of defense turns out to be inadequate, another layer of defense can prevent a security flaw from becoming an exploitable vulnerability and/or limit the consequences of a successful exploit. For example, combining secure programming techniques with secure runtime environments should reduce the likelihood that vulnerabilities remaining in the code at deployment time can be exploited in the operational environment.","title":"6. Practice defense in depth."},{"location":"guide/security-conventions/#7-use-effective-quality-assurance-techniques","text":"Good quality assurance techniques can be effective in identifying and eliminating vulnerabilities. Fuzz testing, penetration testing, and source code audits should all be incorporated as part of an effective quality assurance program. Independent security reviews can lead to more secure systems. External reviewers bring an independent perspective; for example, in identifying and correcting invalid assumptions.","title":"7. Use effective quality assurance techniques."},{"location":"guide/security-conventions/#for-databases","text":"As for SQL, generate your SQL as close to the database - and as far from the user - as possible. UX-side apps should use data-requesting APIs to backend apps that generate SQL, and they should never generate SQL anywhere where a front-end user can see any part of it. If they even have direct connections to the database, you're probably exposed. Never generate SQL in front-end code running on a browser. You're providing a nice interface with already-authenticated connections to hackers that goes directly to your database. To avoid SQL injection attacks, it is best to use prepared statements versus direct execution at your SQL layer, even for queries that run once.","title":"For Databases:"},{"location":"guide/sonarqube-documentations/","text":"Sonarqube is third party opensource tools automatically analyze code and reporting some detect bugs, vulnerabilities, and code smells in your code. Sonarqube workflow This figure explain workflow how sonarqube works. Developer commit/merge their code to a version control system, example here we use git. Add a sonarqube scanner on test pipeline on git ci/cd, this will execute automatically. When test pipeline ready started, code will send to sonarqube service and generate a result statistics. Overview a project listed on dashboard, developer should attention of some bug report or any issues. Any known bugs or vulnerabilities, sonarqube give an advice to correct their code. Overview Report Dashboard 1. Project Menu Default list project will shown as homepage dashboard,this page overview result and general report of all project. 2. Issues Issues will report here, some advice will given from sonarqube related what language code that you write. 3. Rules This rules provide by sonarqube, consist of more programming language rules to detect our project and compare with standard code. 4. Quality Profiles Sonarqube has built in bunch of rules base from programming language. Here we can explore convention code example to reduce or avoid vulnerabilities 5. Quality Grade This page inform you pass criteria of project. By default this built in from sonarqube, a project with status PASSED or FAILED measured from this list. Overview Detail Informations of project name with measure criteria. Passed or failed is depend from quality grade. Project is private, just authenticate user can access it. Informations of bugs, reporting which should be attention to fix of bugs. Vulnerabilities, some potency for attacker or backdoor issue. Different with vulnerability, code consider with less safe need to reviewed. Code smells is inform you measure maintability project or any bad structure from standard code. Covarage test code on project Check duplicating code on project Composition programming language contain on your project Grade Measurement RELIABILITY = A, B, C, D, E SECURITY = A, B, C, D, E SECURITY REVIEW = A, B, C, D, E MAINTAINABILITY = A, B, C, D, E COVERAGE = % Percentage DUPLICATE = % Percentage Credits Sonarqube Documentations","title":"Code Analyzer"},{"location":"guide/sonarqube-documentations/#sonarqube-workflow","text":"This figure explain workflow how sonarqube works. Developer commit/merge their code to a version control system, example here we use git. Add a sonarqube scanner on test pipeline on git ci/cd, this will execute automatically. When test pipeline ready started, code will send to sonarqube service and generate a result statistics. Overview a project listed on dashboard, developer should attention of some bug report or any issues. Any known bugs or vulnerabilities, sonarqube give an advice to correct their code.","title":"Sonarqube workflow"},{"location":"guide/sonarqube-documentations/#overview-report-dashboard","text":"","title":"Overview Report Dashboard"},{"location":"guide/sonarqube-documentations/#1-project-menu","text":"Default list project will shown as homepage dashboard,this page overview result and general report of all project.","title":"1. Project Menu"},{"location":"guide/sonarqube-documentations/#2-issues","text":"Issues will report here, some advice will given from sonarqube related what language code that you write.","title":"2. Issues"},{"location":"guide/sonarqube-documentations/#3-rules","text":"This rules provide by sonarqube, consist of more programming language rules to detect our project and compare with standard code.","title":"3. Rules"},{"location":"guide/sonarqube-documentations/#4-quality-profiles","text":"Sonarqube has built in bunch of rules base from programming language. Here we can explore convention code example to reduce or avoid vulnerabilities","title":"4. Quality Profiles"},{"location":"guide/sonarqube-documentations/#5-quality-grade","text":"This page inform you pass criteria of project. By default this built in from sonarqube, a project with status PASSED or FAILED measured from this list.","title":"5. Quality Grade"},{"location":"guide/sonarqube-documentations/#overview-detail","text":"Informations of project name with measure criteria. Passed or failed is depend from quality grade. Project is private, just authenticate user can access it. Informations of bugs, reporting which should be attention to fix of bugs. Vulnerabilities, some potency for attacker or backdoor issue. Different with vulnerability, code consider with less safe need to reviewed. Code smells is inform you measure maintability project or any bad structure from standard code. Covarage test code on project Check duplicating code on project Composition programming language contain on your project","title":"Overview Detail"},{"location":"guide/sonarqube-documentations/#grade-measurement","text":"RELIABILITY = A, B, C, D, E SECURITY = A, B, C, D, E SECURITY REVIEW = A, B, C, D, E MAINTAINABILITY = A, B, C, D, E COVERAGE = % Percentage DUPLICATE = % Percentage","title":"Grade Measurement"},{"location":"guide/sonarqube-documentations/#credits","text":"Sonarqube Documentations","title":"Credits"},{"location":"guide/standards/","text":"Developer Standard Style Guide Software engineers must use these conventions to make the code more secure also more readable by another engineer: For code style, use the standard such as: Python PEP-0257 - python officially recommends to follow the high-level docstring conventions. This describes how the classes, functions, and codes should be documented and how this really helps for code readability. PEP-3101 - This is the new system for built-in string formatting operations, intended as a replacement for the existing \u2018%\u2019 string formatting operator. PEP-0008 - python coding style guidelines which strongly recommend naming conventions, comments, formatting and lot more... GPSG - Google Python Style Guideline talks about do\u2019s and don'ts for python programs and it explains technically with proof and concepts. Python Anti-Patterns - provides anti-pattern with a technical explanation of best practices and it covers anti-pattern for Django framework also. Reference : The Hitchhikers Guide to Python - the book written by Kenneth and Tany , this book covers lots of recommended best practices from the section called Writing Great Python Code. will use PEP-8 PHP will use PSR-0, PSR-1 or PSR-2 Meaningless / ambiguous variable name should not be used. Default language for any messages such as comments, warning, error, etc. will be using English and not using Bahasa Indonesia Commit Message Guide We use the pattern bellow for commit message: Add: <your commit message> Update: <your commit message> Remove: <your commit message> Fix: <your commit message> Add : for feature additions. Update : for feature update. Remove : for feature dropping, etc. Fix : for bug fixing, style, etc. Comments For every code and function, there must be a comment. Refer to PEP-8, the comment must: Complete sentence, its first word should be capitalized, unless it is an identifier that begins with a lower case letter If a comment is short, the period at the end can be omitted Use two spaces after a sentence-ending period Use English","title":"Standards"},{"location":"guide/standards/#developer-standard","text":"","title":"Developer Standard"},{"location":"guide/standards/#style-guide","text":"Software engineers must use these conventions to make the code more secure also more readable by another engineer: For code style, use the standard such as: Python PEP-0257 - python officially recommends to follow the high-level docstring conventions. This describes how the classes, functions, and codes should be documented and how this really helps for code readability. PEP-3101 - This is the new system for built-in string formatting operations, intended as a replacement for the existing \u2018%\u2019 string formatting operator. PEP-0008 - python coding style guidelines which strongly recommend naming conventions, comments, formatting and lot more... GPSG - Google Python Style Guideline talks about do\u2019s and don'ts for python programs and it explains technically with proof and concepts. Python Anti-Patterns - provides anti-pattern with a technical explanation of best practices and it covers anti-pattern for Django framework also. Reference : The Hitchhikers Guide to Python - the book written by Kenneth and Tany , this book covers lots of recommended best practices from the section called Writing Great Python Code. will use PEP-8 PHP will use PSR-0, PSR-1 or PSR-2 Meaningless / ambiguous variable name should not be used. Default language for any messages such as comments, warning, error, etc. will be using English and not using Bahasa Indonesia","title":"Style Guide"},{"location":"guide/standards/#commit-message-guide","text":"We use the pattern bellow for commit message: Add: <your commit message> Update: <your commit message> Remove: <your commit message> Fix: <your commit message> Add : for feature additions. Update : for feature update. Remove : for feature dropping, etc. Fix : for bug fixing, style, etc.","title":"Commit Message Guide"},{"location":"guide/standards/#comments","text":"For every code and function, there must be a comment. Refer to PEP-8, the comment must: Complete sentence, its first word should be capitalized, unless it is an identifier that begins with a lower case letter If a comment is short, the period at the end can be omitted Use two spaces after a sentence-ending period Use English","title":"Comments"},{"location":"guide/task-board/","text":"On gitlab we manage workitems and task from issues. You can create any issue and assign to yourself or another person on your teams. Once you create an issue, dont forget set issues on task board to track progress report. On gitlab, We provide important boards to manage your progress issues. Defined task board step: Task Board Step 1. Open First time you create issue, default is on open board. When issue on open board may still discussing or as a backlog. 2. To Do Plan and manage a strategy to resolve your issue. 3. Doing Start activity to resolving issues. 4. Done Completed task and ready to deliver(as ussually on staging deployment). 5. Tested Checked and tested by product owner 6. Closed Closes issue when ready on Production (autoclose issues by tag close # on descriptions pull request master). Attention Keep update and track your progress issue. We warn a notify when you lefting issue for 5 days without any progress.","title":"Task Board"},{"location":"guide/task-board/#task-board-step","text":"","title":"Task Board Step"},{"location":"guide/task-board/#1-open","text":"First time you create issue, default is on open board. When issue on open board may still discussing or as a backlog.","title":"1. Open"},{"location":"guide/task-board/#2-to-do","text":"Plan and manage a strategy to resolve your issue.","title":"2. To Do"},{"location":"guide/task-board/#3-doing","text":"Start activity to resolving issues.","title":"3. Doing"},{"location":"guide/task-board/#4-done","text":"Completed task and ready to deliver(as ussually on staging deployment).","title":"4. Done"},{"location":"guide/task-board/#5-tested","text":"Checked and tested by product owner","title":"5. Tested"},{"location":"guide/task-board/#6-closed","text":"Closes issue when ready on Production (autoclose issues by tag close # on descriptions pull request master). Attention Keep update and track your progress issue. We warn a notify when you lefting issue for 5 days without any progress.","title":"6. Closed"},{"location":"guide/test-minimalist/","text":"This page will give you some tutorial to create a test minimalist on your project. Magic Tricks of Testing (summary) Sandy Metz Incoming Incoming Query (Assert Result) class Whell : def __init__ ( self , rim , tire ): self . rim = rim self . tire = tire def diameter ( self ): return self . rim + ( self . tire * 2 ) class TestWheel : def test_calculates_diameter ( self ): wheel = Whell ( 26 , 1.5 ) assert wheel . diameter () == 29 ## \u2b06 Test the return value (result) Incoming (private) Query class Wheel : def __init__ ( self , rim , tire ): self . rim = rim self . tire = tire def diameter ( self ): return self . rim + ( self . tire * 2 ) class Gear : def __init__ ( self , chainring , cog , wheel ): self . chainring = chainring self . cog = cog self . wheel = wheel def _ratio ( self ): return self . chainring / self . cog def gear_inches ( self ): return self . _ratio () * self . wheel . diameter () # call private method class TestGear : def test_calculates_gear_inches ( self ): wheel = Wheel ( 26 , 1.5 ) gear = Gear ( 52 , 11 , wheel ) assert gear . gear_inches () == pytest . approx ( 137 , 0.01 ) ## \u2b06 test the interface, not the implementation ## (without private function) Incoming Command (Assert public side effect) class Gear : def __init__ ( self ): self . cog = 0 def get_cog ( self ): return self . cog def set_cog ( self , cog ): self . cog = cog class TestGear : def test_set_cog ( self ): gear = Gear () gear . set_cog ( 27 ) assert gear . cog == 27 ## \u2b06 assert public side effect To Self Sent to self (Ignore) class Wheel : def __init__ ( self , rim , tire ): self . rim = rim self . tire = tire def diameter ( self ): return self . rim + ( self . tire * 2 ) class Gear : def __init__ ( self , chainring , cog , wheel ): self . chainring = chainring self . cog = cog self . wheel = wheel def _ratio ( self ): return self . chainring / self . cog def gear_inches ( self ): return self . _ratio () * self . wheel . diameter () # call private method class TestGear : def test_calculates_ratio ( self ): ## \u2b05 test private method wheel = Wheel ( 26 , 1.5 ) gear = Gear ( 52 , 11 , wheel ) assert gear . _ratio == pytest . approx ( 4.7 , 0.01 ) ## call to self ## \u2b06 this is redundant test. `gear_inches` return value is proof enough ## don\u2019t test private method (assert / expectation). Don\u2019t! Outgoing Outgoing Query (Ignore) class Wheel : def __init__ ( self , rim , tire ): self . rim = rim self . tire = tire def diameter ( self ): return self . rim + ( self . tire * 2 ) class Gear : def __init__ ( self , chainring , cog , wheel ): self . chainring = chainring self . cog = cog self . wheel = wheel def _ratio ( self ): return self . chainring / self . cog def gear_inches ( self ): return self . _ratio () * self . wheel . diameter () # call other class (outgoing) class TestGear : def test_calculates_gear_inches ( self ): wheel = Wheel ( 26 , 1.5 ) gear = Gear ( 52 , 11 , wheel ) # test outgoing command `wheel.diameter()` assert gear . wheel . diameter () == 29 ## \u2b06 this is already done in `Wheel` test (redundant) Outgoing Command (Expect to send) # assert someting (side effect) in far away. # This is not `Gear` responsibility. # This is an integration test, not unit test. Credits https://www.youtube.com/watch?v=URSWYvyc42M https://speakerdeck.com/skmetz/magic-tricks-of-testing-railsconf","title":"Testing Minimalist"},{"location":"guide/test-minimalist/#magic-tricks-of-testing-summary-sandy-metz","text":"","title":"Magic Tricks of Testing (summary) Sandy Metz"},{"location":"guide/test-minimalist/#incoming","text":"","title":"Incoming"},{"location":"guide/test-minimalist/#incoming-query-assert-result","text":"class Whell : def __init__ ( self , rim , tire ): self . rim = rim self . tire = tire def diameter ( self ): return self . rim + ( self . tire * 2 ) class TestWheel : def test_calculates_diameter ( self ): wheel = Whell ( 26 , 1.5 ) assert wheel . diameter () == 29 ## \u2b06 Test the return value (result)","title":"Incoming Query  (Assert Result)"},{"location":"guide/test-minimalist/#incoming-private-query","text":"class Wheel : def __init__ ( self , rim , tire ): self . rim = rim self . tire = tire def diameter ( self ): return self . rim + ( self . tire * 2 ) class Gear : def __init__ ( self , chainring , cog , wheel ): self . chainring = chainring self . cog = cog self . wheel = wheel def _ratio ( self ): return self . chainring / self . cog def gear_inches ( self ): return self . _ratio () * self . wheel . diameter () # call private method class TestGear : def test_calculates_gear_inches ( self ): wheel = Wheel ( 26 , 1.5 ) gear = Gear ( 52 , 11 , wheel ) assert gear . gear_inches () == pytest . approx ( 137 , 0.01 ) ## \u2b06 test the interface, not the implementation ## (without private function)","title":"Incoming (private) Query"},{"location":"guide/test-minimalist/#incoming-command-assert-public-side-effect","text":"class Gear : def __init__ ( self ): self . cog = 0 def get_cog ( self ): return self . cog def set_cog ( self , cog ): self . cog = cog class TestGear : def test_set_cog ( self ): gear = Gear () gear . set_cog ( 27 ) assert gear . cog == 27 ## \u2b06 assert public side effect","title":"Incoming Command (Assert public side effect)"},{"location":"guide/test-minimalist/#to-self","text":"","title":"To Self"},{"location":"guide/test-minimalist/#sent-to-self-ignore","text":"class Wheel : def __init__ ( self , rim , tire ): self . rim = rim self . tire = tire def diameter ( self ): return self . rim + ( self . tire * 2 ) class Gear : def __init__ ( self , chainring , cog , wheel ): self . chainring = chainring self . cog = cog self . wheel = wheel def _ratio ( self ): return self . chainring / self . cog def gear_inches ( self ): return self . _ratio () * self . wheel . diameter () # call private method class TestGear : def test_calculates_ratio ( self ): ## \u2b05 test private method wheel = Wheel ( 26 , 1.5 ) gear = Gear ( 52 , 11 , wheel ) assert gear . _ratio == pytest . approx ( 4.7 , 0.01 ) ## call to self ## \u2b06 this is redundant test. `gear_inches` return value is proof enough ## don\u2019t test private method (assert / expectation). Don\u2019t!","title":"Sent to self (Ignore)"},{"location":"guide/test-minimalist/#outgoing","text":"","title":"Outgoing"},{"location":"guide/test-minimalist/#outgoing-query-ignore","text":"class Wheel : def __init__ ( self , rim , tire ): self . rim = rim self . tire = tire def diameter ( self ): return self . rim + ( self . tire * 2 ) class Gear : def __init__ ( self , chainring , cog , wheel ): self . chainring = chainring self . cog = cog self . wheel = wheel def _ratio ( self ): return self . chainring / self . cog def gear_inches ( self ): return self . _ratio () * self . wheel . diameter () # call other class (outgoing) class TestGear : def test_calculates_gear_inches ( self ): wheel = Wheel ( 26 , 1.5 ) gear = Gear ( 52 , 11 , wheel ) # test outgoing command `wheel.diameter()` assert gear . wheel . diameter () == 29 ## \u2b06 this is already done in `Wheel` test (redundant)","title":"Outgoing Query (Ignore)"},{"location":"guide/test-minimalist/#outgoing-command-expect-to-send","text":"# assert someting (side effect) in far away. # This is not `Gear` responsibility. # This is an integration test, not unit test.","title":"Outgoing Command (Expect to send)"},{"location":"guide/test-minimalist/#credits","text":"https://www.youtube.com/watch?v=URSWYvyc42M https://speakerdeck.com/skmetz/magic-tricks-of-testing-railsconf","title":"Credits"}]}